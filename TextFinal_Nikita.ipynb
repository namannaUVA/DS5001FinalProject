{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_AGPE3X1bnnADyi91iirQWXu3SQxZQDb","timestamp":1682200687962}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# A Comparison of Early American and British Science Fiction Novels\n","\n","## Humaira Halim, Nick Kalinowski, Nikita Amanna"],"metadata":{"id":"JqNYqqnFPJm2"}},{"cell_type":"code","source":["OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n","\n","import pandas as pd\n","import numpy as np\n","from glob import glob\n","import re\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('tagsets')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","!pip install plotly_express\n","import plotly_express as px\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkoHLptUPS-S","executionInfo":{"status":"ok","timestamp":1683217463092,"user_tz":240,"elapsed":44351,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"f5dcd47b-fd4e-43c2-9d6b-e492b6a9d34d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting plotly_express\n","  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.22.4)\n","Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (5.13.1)\n","Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (0.5.3)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.10.1)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (0.13.5)\n","Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.5.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5->plotly_express) (1.16.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly_express) (8.2.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->plotly_express) (23.1)\n","Installing collected packages: plotly_express\n","Successfully installed plotly_express-0.4.1\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","\n","class TextParser():\n","    \"\"\"\n","    A class to parse a single Gutenberg-type text files into a TOKENS dataframe with\n","    an OHCO index. Also has methods to extract a VOCAB table, although vocabulary\n","    tables out to be generated at the corpus level.\n","    \n","    Sample parameter values:\n","    ohco_pats = [\n","        ('chapter', r\"^\\s*(chapter|letter)\\s+(\\d+)\", 'm')    \n","    ]\n","    clip_pats = [\n","        r'START OF GUTENBERG PROJECT', \n","        r'^\\s*THE END'\n","    ]\n","    \"\"\"\n","\n","    # TODO: Make these private\n","    src_imported:bool = False       \n","    src_clipped:bool = False\n","    src_col_suffix:str ='_str'\n","\n","    join_pat:str = r'\\n'\n","    strip_hyphens:bool = False\n","    strip_whitespace:bool = False\n","    verbose:bool = False\n","\n","    stanford_pos_model:str = \"english-bidirectional-distsim.tagger\"\n","    stanford_pos_model_path = None\n","        \n","    # We assume all OHCOs have sentences and tokens\n","    # and that there are terminal in the list.\n","    ohco_pats:[] = [\n","        ('para', r\"\\n\\n\", 'd'),\n","        ('sent', r\"[.?!;:]+\", 'd'),\n","        ('token', r\"[\\s',-]+\", 'd')\n","    ]\n","        \n","    _ohco_type:{} = {\n","        'd': '_num',\n","        'm': '_id'\n","    }\n","        \n","    def __init__(self, src_file:str, ohco_pats:[], clip_pats:[], use_nltk=True):\n","        \"\"\"Initialize the object and extract config info. If using NLTK, download resources.\"\"\"\n","        self.src_file = src_file            \n","        self.clip_pats = clip_pats # TODO: Validate\n","        self.ohco_pats = ohco_pats + self.ohco_pats # TODO: Validate\n","        self.OHCO = [item[0]+self._ohco_type[item[2]] for item in self.ohco_pats]\n","        self.ohco_names = [item[0] for item in self.ohco_pats]\n","        self.use_nltk = use_nltk\n","\n","        if self.use_nltk:\n","            # Override the last two OHCO items\n","            self.ohco_pats[-2] = ('sent', None, 'nltk')\n","            self.ohco_pats[-1] = ('token', None, 'nltk')\n","            # Make sure you have the NLTK stuff\n","            for package in [\n","                'tokenizers/punkt', \n","                'taggers/averaged_perceptron_tagger', \n","                'corpora/stopwords', \n","                'help/tagsets'\n","            ]:\n","                if self.verbose: print(\"Checking\", package)\n","                try:\n","                    nltk.data.find(package)\n","                except IndexError:\n","                    nltk.download(package)\n","            \n","    def import_source(self, strip:bool=True, char_encoding:str=\"utf-8-sig\"):\n","        \"\"\"Convert a raw text file into a dataframe of lines.\"\"\"\n","        if self.verbose: print(\"Importing \", self.src_file)\n","        text_lines = open(self.src_file,'r', encoding=char_encoding).readlines()\n","        self.LINES = pd.DataFrame({'line_str':text_lines})\n","        self.LINES.index.name = 'line_id'\n","        if strip:\n","            self.LINES.line_str = self.LINES.line_str.str.strip()\n","        self.src_imported = True\n","        if self.verbose: print(\"Clipping text\")\n","        self._clip_lines()\n","        return self        \n","\n","    def _clip_lines(self):\n","        \"\"\"Remove cruft lines from beginning and/or end of file.\"\"\"\n","        start_pat = self.clip_pats[0]\n","        end_pat = self.clip_pats[1]\n","        start = self.LINES.line_str.str.contains(start_pat, regex=True)\n","        end = self.LINES.line_str.str.contains(end_pat, regex=True)\n","        try:\n","            start_line_num = self.LINES.loc[start].index[0]\n","        except IndexError:\n","            raise ValueError(\"Clip start pattern not found.\")            \n","        try:\n","            end_line_num = self.LINES.loc[end].index[0]\n","        except IndexError:\n","            raise ValueError(\"Clip end pattern not found.\")\n","        self.LINES = self.LINES.loc[start_line_num + 1 : end_line_num - 2]\n","        self.src_clipped == True\n","        \n","    def parse_tokens(self):\n","        \"\"\"Convert lines to tokens based on OHCO.\"\"\"\n","        if self.src_imported:\n","\n","            # Start with the LINES df\n","            self.TOKENS = self.LINES.copy()\n","\n","            # Walk through each level of the OHCO to build out TOKENS\n","            for i, level in enumerate(self.OHCO):\n","\n","                if self.verbose: print(f\"Parsing OHCO level {i} {level}\", end=' ')\n","\n","                # Define level-specific variables\n","                parse_type = self.ohco_pats[i][2]\n","                div_name = self.ohco_pats[i][0]\n","                div_pat = self.ohco_pats[i][1]\n","                if i == 0:\n","                    src_div_name = 'line'\n","                else:\n","                    src_div_name = self.ohco_names[i - 1] \n","                src_col = f\"{src_div_name}{self.src_col_suffix}\"\n","                dst_col = f\"{div_name}{self.src_col_suffix}\"\n","\n","                # By Milestone\n","                if parse_type == 'm':\n","                    if self.verbose: print(f\"by milestone {div_pat}\")\n","                    div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True) # TODO: Parametize case\n","                    self.TOKENS.loc[div_lines, div_name] = [i+1 for i in range(self.TOKENS.loc[div_lines].shape[0])]\n","                    self.TOKENS[div_name] = self.TOKENS[div_name].ffill()\n","                    self.TOKENS = self.TOKENS.loc[~self.TOKENS[div_name].isna()] \n","                    self.TOKENS = self.TOKENS.loc[~div_lines] \n","                    self.TOKENS[div_name] = self.TOKENS[div_name].astype('int')\n","                    self.TOKENS = self.TOKENS.groupby(self.ohco_names[:i+1], group_keys=True)[src_col]\\\n","                        .apply(lambda x: '\\n'.join(x)).to_frame(dst_col)\n","\n","                    # print(self.TOKENS[dst_col].str.count(r'\\n\\n'))\n","                    print(src_col, dst_col)\n","                    print(self.TOKENS.columns)\n","\n","\n","                # By Delimitter\n","                elif parse_type == 'd':\n","                    if self.verbose: print(f\"by delimitter {div_pat}\")\n","                    self.TOKENS = self.TOKENS[src_col].str.split(div_pat, expand=True).stack().to_frame(dst_col)\n","                \n","                # By NLTK \n","                elif parse_type == 'nltk':\n","                    if self.verbose: print(f\"by NLTK model\")\n","\n","                    if level == 'sent_num':\n","                        self.TOKENS = self.TOKENS.para_str\\\n","                                .apply(lambda x: pd.Series(nltk.sent_tokenize(x), dtype='string'))\\\n","                                .stack()\\\n","                                .to_frame('sent_str')\n","                    \n","                    if level == 'token_num':\n","                        if self.strip_hyphens == True:\n","                            self.TOKENS.sent_str = self.TOKENS.sent_str.str.replace(r\"-\", ' ')\n","                        if self.strip_whitespace == True:\n","                            self.TOKENS = self.TOKENS.sent_str\\\n","                                    .apply(lambda x: pd.Series(\n","                                            nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)),\n","                                            dtype='object'\n","                                        )\n","                                    )\n","                        else:\n","                            self.TOKENS = self.TOKENS.sent_str\\\n","                                    .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\n","                        self.TOKENS = self.TOKENS.stack().to_frame('pos_tuple')\n","                        self.TOKENS['pos'] = self.TOKENS.pos_tuple.apply(lambda x: x[1])\n","                        self.TOKENS['token_str'] = self.TOKENS.pos_tuple.apply(lambda x: x[0])\n","                        self.TOKENS['term_str'] = self.TOKENS.token_str.str.lower()   \n","        \n","                else:\n","                    raise ValueError(f\"Invalid parse option: {parse_type}.\")\n","\n","                # After creating the current OHCO level\n","                self.TOKENS.index.names = self.OHCO[:i+1]\n","\n","            # After iterating through the OHCO\n","\n","            # Not sure if needed anymore\n","            # self.TOKENS[dst_col] = self.TOKENS[dst_col].str.strip()\n","            # self.TOKENS[dst_col] = self.TOKENS[dst_col].str.replace(self.join_pat, ' ', regex=True)\n","            # self.TOKENS = self.TOKENS[~self.TOKENS[dst_col].str.contains(r'^\\s*$', regex=True)]\n","\n","            if not self.use_nltk:\n","                self.TOKENS['term_str'] = self.TOKENS.token_str.str.replace(r'[\\W_]+', '', regex=True).str.lower()  \n","            else:\n","                punc_pos = ['$', \"''\", '(', ')', ',', '--', '.', ':', '``']\n","                self.TOKENS['term_str'] = self.TOKENS[~self.TOKENS.pos.isin(punc_pos)].token_str\\\n","                    .str.replace(r'[\\W_]+', '', regex=True).str.lower()  \n","            \n","        else:\n","            raise RuntimeError(\"Source not imported. Please run .import_source()\")\n","\n","    def extract_vocab(self):\n","        \"\"\"This should also be done at the corpus level.\"\"\"\n","        self.VOCAB = self.TOKENS.term_str.value_counts().to_frame('n')\n","        self.VOCAB.index.name = 'term_str'\n","        self.VOCAB['n_chars'] = self.VOCAB.index.str.len()\n","        self.VOCAB['p'] = self.VOCAB['n'] / self.VOCAB['n'].sum()\n","        self.VOCAB['s'] = 1 / self.VOCAB['p']\n","        self.VOCAB['i'] = np.log2(self.VOCAB['s']) # Same as negative log probability (i.e. log likelihood)\n","        self.VOCAB['h'] = self.VOCAB['p'] * self.VOCAB['i']\n","        self.H = self.VOCAB['h'].sum()\n","        return self\n","\n","    def annotate_vocab(self):\n","        \"\"\"This should be done at the corpus level.\"\"\"\n","        # Stopwords\n","        # Max POS\n","        # POS variability\n","        # Porter Stems\n","        pass\n","\n","    def extract_pos_data(self):\n","        # TODO: Create dataframe for POS info, including Penn Treebank info\n","        pass\n","\n","    def extract_named_entities(self):\n","        # TODO: Create dataframe of named entities\n","        pass\n","\n","    def gather_tokens(self, level=0, grouping_col='term_str', cat_sep=' '):\n","        \"\"\"Gather tokens into strings for arbitrary OHCO level.\"\"\"\n","        max_level = len(self.OHCO) - 2 # Can't gather tokens at the token level :)\n","        if level > max_level:\n","            raise ValueError(f\"Level {level} too high. Try between 0 and {max_level}\")\n","        else:\n","            level_name = self.OHCO[level].split('_')[0]\n","            idx = self.TOKENS.index.names[:level+1]\n","            return self.TOKENS.groupby(idx)[grouping_col].apply(lambda x: x.str.cat(sep=cat_sep))\\\n","                .to_frame(f'{level_name}_str')\n","\n","\n","if __name__ == '__main__':\n","    pass"],"metadata":{"id":"Hp1oJm_OPzzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file1 = open('/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/201.txt', 'r')\n","\n","file2 = open('/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/84.txt', 'r')\n","\n","file3 = open('/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/5230.txt', 'r')\n","\n","file4 = open('/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/18247.txt', 'r')\n","\n","book_nums = [201, 84, 5230, 18247]\n","\n","file_paths = ['/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/201.txt', '/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/84.txt', '/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/5230.txt', '/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/18247.txt']\n","\n","author_list = ['Edwin A. Abbot', 'Mary Wollstonecraft Shelley', 'H. G. Wells', 'Mary Wollstonecraft Shelley']\n","\n","title_list = ['Flatland: A Romance of Many Dimensions', 'Frankenstein', 'The Invisible Man', 'The Last Man']\n","\n","roman = '[IVXLCM]+'\n","caps = \"[A-Z';, -]+\"\n","nums = \"[0-9;,-]+\"\n","\n","ohco_pat_list = [\n","    (201,   rf\"^{nums}\\.*$\"),\n","    (84,   r\"^\\s*(?:Chapter|Letter)\\s+\\w+\"),\n","    (5230,  rf\"^({roman}|{caps})\\.*$\"),\n","    (18247, r\"^\\s*(?:chapter|letter)\\s+\\w+\")\n","]\n","\n","LIB = pd.DataFrame({'book_id':book_nums, 'source_file_path':file_paths, 'author':author_list, 'title':title_list}).set_index('book_id')\n","\n","LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n","\n","LIB"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"id":"1meDHjOLS0pX","executionInfo":{"status":"ok","timestamp":1683217512839,"user_tz":240,"elapsed":1013,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"4ad9b777-29fe-4f67-c882-f7928c4b6a44"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          source_file_path  \\\n","book_id                                                      \n","201      /content/drive/MyDrive/Graduate/DS 5001 Final ...   \n","84       /content/drive/MyDrive/Graduate/DS 5001 Final ...   \n","5230     /content/drive/MyDrive/Graduate/DS 5001 Final ...   \n","18247    /content/drive/MyDrive/Graduate/DS 5001 Final ...   \n","\n","                              author                                   title  \\\n","book_id                                                                        \n","201                   Edwin A. Abbot  Flatland: A Romance of Many Dimensions   \n","84       Mary Wollstonecraft Shelley                            Frankenstein   \n","5230                     H. G. Wells                       The Invisible Man   \n","18247    Mary Wollstonecraft Shelley                            The Last Man   \n","\n","                           chap_regex  \n","book_id                                \n","201                    ^[0-9;,-]+\\.*$  \n","84       ^\\s*(?:Chapter|Letter)\\s+\\w+  \n","5230     ^([IVXLCM]+|[A-Z';, -]+)\\.*$  \n","18247    ^\\s*(?:chapter|letter)\\s+\\w+  "],"text/html":["\n","  <div id=\"df-ba8c5a1b-cebb-4199-8afd-d5b58e4cfa0c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source_file_path</th>\n","      <th>author</th>\n","      <th>title</th>\n","      <th>chap_regex</th>\n","    </tr>\n","    <tr>\n","      <th>book_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>201</th>\n","      <td>/content/drive/MyDrive/Graduate/DS 5001 Final ...</td>\n","      <td>Edwin A. Abbot</td>\n","      <td>Flatland: A Romance of Many Dimensions</td>\n","      <td>^[0-9;,-]+\\.*$</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>/content/drive/MyDrive/Graduate/DS 5001 Final ...</td>\n","      <td>Mary Wollstonecraft Shelley</td>\n","      <td>Frankenstein</td>\n","      <td>^\\s*(?:Chapter|Letter)\\s+\\w+</td>\n","    </tr>\n","    <tr>\n","      <th>5230</th>\n","      <td>/content/drive/MyDrive/Graduate/DS 5001 Final ...</td>\n","      <td>H. G. Wells</td>\n","      <td>The Invisible Man</td>\n","      <td>^([IVXLCM]+|[A-Z';, -]+)\\.*$</td>\n","    </tr>\n","    <tr>\n","      <th>18247</th>\n","      <td>/content/drive/MyDrive/Graduate/DS 5001 Final ...</td>\n","      <td>Mary Wollstonecraft Shelley</td>\n","      <td>The Last Man</td>\n","      <td>^\\s*(?:chapter|letter)\\s+\\w+</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba8c5a1b-cebb-4199-8afd-d5b58e4cfa0c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ba8c5a1b-cebb-4199-8afd-d5b58e4cfa0c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ba8c5a1b-cebb-4199-8afd-d5b58e4cfa0c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["LIB.to_csv('/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/lib.csv')  "],"metadata":{"id":"O2IGic-0kuY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_collection(LIB):\n","\n","    clip_pats = [\n","        r\"\\*\\*\\*\\s*START OF\",\n","        r\"\\*\\*\\*\\s*END OF\"\n","    ]\n","\n","    books = []\n","    for book_id in LIB.index:\n","\n","        # Announce\n","        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n","\n","        # Define vars\n","        chap_regex = LIB.loc[book_id].chap_regex\n","        ohco_pats = [('chap', chap_regex, 'm')]\n","        src_file_path = LIB.loc[book_id].source_file_path\n","\n","        # Create object\n","        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n","\n","        # Define parameters\n","        text.verbose = True\n","        text.strip_hyphens = True\n","        text.strip_whitespace = True\n","\n","        # Parse\n","        text.import_source().parse_tokens();\n","\n","        # Name things\n","        text.TOKENS['book_id'] = book_id\n","        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n","\n","        # Add to list\n","        books.append(text.TOKENS)\n","        \n","    # Combine into a single dataframe\n","    CORPUS = pd.concat(books).sort_index()\n","\n","    # Clean up\n","    del(books)\n","    del(text)\n","        \n","    print(\"Done\")\n","        \n","    return CORPUS"],"metadata":{"id":"9fsyMID-XQCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CORPUS = tokenize_collection(LIB)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urHjhahHXQ2F","executionInfo":{"status":"ok","timestamp":1683217736660,"user_tz":240,"elapsed":34404,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"9870df97-019d-4e06-80e2-350f498f7780"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing 201 Flatland: A Romance of Many Dimensions\n","Importing  /content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/201.txt\n","Clipping text\n","Parsing OHCO level 0 chap_id by milestone ^[0-9;,-]+\\.*$\n","line_str chap_str\n","Index(['chap_str'], dtype='object')\n","Parsing OHCO level 1 para_num by delimitter \\n\\n\n","Parsing OHCO level 2 sent_num by NLTK model\n","Parsing OHCO level 3 token_num by NLTK model\n","Tokenizing 84 Frankenstein\n","Importing  /content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/84.txt\n","Clipping text\n","Parsing OHCO level 0 chap_id by milestone ^\\s*(?:Chapter|Letter)\\s+\\w+\n","line_str chap_str\n","Index(['chap_str'], dtype='object')\n","Parsing OHCO level 1 para_num by delimitter \\n\\n\n","Parsing OHCO level 2 sent_num by NLTK model\n","Parsing OHCO level 3 token_num by NLTK model\n","Tokenizing 5230 The Invisible Man\n","Importing  /content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/5230.txt\n","Clipping text\n","Parsing OHCO level 0 chap_id by milestone ^([IVXLCM]+|[A-Z';, -]+)\\.*$\n","line_str chap_str\n","Index(['chap_str'], dtype='object')\n","Parsing OHCO level 1 para_num by delimitter \\n\\n\n","Parsing OHCO level 2 sent_num by NLTK model\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-7c8cf6f00ae2>:129: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n","  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True) # TODO: Parametize case\n"]},{"output_type":"stream","name":"stdout","text":["Parsing OHCO level 3 token_num by NLTK model\n","Tokenizing 18247 The Last Man\n","Importing  /content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/18247.txt\n","Clipping text\n","Parsing OHCO level 0 chap_id by milestone ^\\s*(?:chapter|letter)\\s+\\w+\n","line_str chap_str\n","Index(['chap_str'], dtype='object')\n","Parsing OHCO level 1 para_num by delimitter \\n\\n\n","Parsing OHCO level 2 sent_num by NLTK model\n","Parsing OHCO level 3 token_num by NLTK model\n","Done\n"]}]},{"cell_type":"code","source":["CORPUS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"kaG9HYUlYLg4","executionInfo":{"status":"ok","timestamp":1683217756885,"user_tz":240,"elapsed":222,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"d571df01-2737-4ba6-bb4b-cc4e5dba99a7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                     pos_tuple  pos  \\\n","book_id chap_id para_num sent_num token_num                           \n","84      29      0        0        0                 (_To, NNP)  NNP   \n","                                  1                (Mrs., NNP)  NNP   \n","                                  2            (Saville,, NNP)  NNP   \n","                                  3           (England._, NNP)  NNP   \n","                1        0        0                 (St., NNP)  NNP   \n","...                                                        ...  ...   \n","18247   1       1241     8        39         (Verney—the, NNP)  NNP   \n","                                  40               (LAST, NNP)  NNP   \n","                                  41               (MAN., NNP)  NNP   \n","                1242     0        0                  (THE, DT)   DT   \n","                                  1                (END., NNP)  NNP   \n","\n","                                              token_str   term_str  \n","book_id chap_id para_num sent_num token_num                         \n","84      29      0        0        0                 _To         to  \n","                                  1                Mrs.        mrs  \n","                                  2            Saville,    saville  \n","                                  3           England._    england  \n","                1        0        0                 St.         st  \n","...                                                 ...        ...  \n","18247   1       1241     8        39         Verney—the  verneythe  \n","                                  40               LAST       last  \n","                                  41               MAN.        man  \n","                1242     0        0                 THE        the  \n","                                  1                END.        end  \n","\n","[330840 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-9688c735-9885-4f8d-b3b6-6b8a3aa24202\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>pos_tuple</th>\n","      <th>pos</th>\n","      <th>token_str</th>\n","      <th>term_str</th>\n","    </tr>\n","    <tr>\n","      <th>book_id</th>\n","      <th>chap_id</th>\n","      <th>para_num</th>\n","      <th>sent_num</th>\n","      <th>token_num</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">84</th>\n","      <th rowspan=\"5\" valign=\"top\">29</th>\n","      <th rowspan=\"4\" valign=\"top\">0</th>\n","      <th rowspan=\"4\" valign=\"top\">0</th>\n","      <th>0</th>\n","      <td>(_To, NNP)</td>\n","      <td>NNP</td>\n","      <td>_To</td>\n","      <td>to</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(Mrs., NNP)</td>\n","      <td>NNP</td>\n","      <td>Mrs.</td>\n","      <td>mrs</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(Saville,, NNP)</td>\n","      <td>NNP</td>\n","      <td>Saville,</td>\n","      <td>saville</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(England._, NNP)</td>\n","      <td>NNP</td>\n","      <td>England._</td>\n","      <td>england</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <th>0</th>\n","      <th>0</th>\n","      <td>(St., NNP)</td>\n","      <td>NNP</td>\n","      <td>St.</td>\n","      <td>st</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">18247</th>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th rowspan=\"3\" valign=\"top\">1241</th>\n","      <th rowspan=\"3\" valign=\"top\">8</th>\n","      <th>39</th>\n","      <td>(Verney—the, NNP)</td>\n","      <td>NNP</td>\n","      <td>Verney—the</td>\n","      <td>verneythe</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>(LAST, NNP)</td>\n","      <td>NNP</td>\n","      <td>LAST</td>\n","      <td>last</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>(MAN., NNP)</td>\n","      <td>NNP</td>\n","      <td>MAN.</td>\n","      <td>man</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">1242</th>\n","      <th rowspan=\"2\" valign=\"top\">0</th>\n","      <th>0</th>\n","      <td>(THE, DT)</td>\n","      <td>DT</td>\n","      <td>THE</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(END., NNP)</td>\n","      <td>NNP</td>\n","      <td>END.</td>\n","      <td>end</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>330840 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9688c735-9885-4f8d-b3b6-6b8a3aa24202')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9688c735-9885-4f8d-b3b6-6b8a3aa24202 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9688c735-9885-4f8d-b3b6-6b8a3aa24202');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["CORPUS.to_csv('/content/drive/MyDrive/Graduate/DS 5001 Final Project/Text Analytics Final/corpus.csv')"],"metadata":{"id":"SBwLDgJ-mMRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["invisible_man = CORPUS.query('book_id == 5230')\n","\n","# invisible_man = invisible_man.iloc[:24880,:]\n","\n","invisible_man"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"yK_ye31jdKCs","executionInfo":{"status":"ok","timestamp":1683217830795,"user_tz":240,"elapsed":225,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"f83f2687-58ec-4376-bdcd-7ac3ad9db0a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  pos_tuple  pos token_str  \\\n","book_id chap_id para_num sent_num token_num                                  \n","5230    1       0        0        0               (THE, DT)   DT       THE   \n","                                  1          (STRANGE, NNP)  NNP   STRANGE   \n","                                  2            (MAN’S, NNP)  NNP     MAN’S   \n","                                  3          (ARRIVAL, NNP)  NNP   ARRIVAL   \n","                1        0        0               (The, DT)   DT       The   \n","...                                                     ...  ...       ...   \n","        47      11       2        5                (of, IN)   IN        of   \n","                                  6             (them, PRP)  PRP      them   \n","                                  7             (until, IN)   IN     until   \n","                                  8               (he, PRP)  PRP        he   \n","                                  9             (dies., VB)   VB     dies.   \n","\n","                                            term_str  \n","book_id chap_id para_num sent_num token_num           \n","5230    1       0        0        0              the  \n","                                  1          strange  \n","                                  2             mans  \n","                                  3          arrival  \n","                1        0        0              the  \n","...                                              ...  \n","        47      11       2        5               of  \n","                                  6             them  \n","                                  7            until  \n","                                  8               he  \n","                                  9             dies  \n","\n","[48500 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-51486ea8-6801-46fa-a296-9c982abd22bc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>pos_tuple</th>\n","      <th>pos</th>\n","      <th>token_str</th>\n","      <th>term_str</th>\n","    </tr>\n","    <tr>\n","      <th>book_id</th>\n","      <th>chap_id</th>\n","      <th>para_num</th>\n","      <th>sent_num</th>\n","      <th>token_num</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"11\" valign=\"top\">5230</th>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th rowspan=\"4\" valign=\"top\">0</th>\n","      <th rowspan=\"4\" valign=\"top\">0</th>\n","      <th>0</th>\n","      <td>(THE, DT)</td>\n","      <td>DT</td>\n","      <td>THE</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(STRANGE, NNP)</td>\n","      <td>NNP</td>\n","      <td>STRANGE</td>\n","      <td>strange</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(MAN’S, NNP)</td>\n","      <td>NNP</td>\n","      <td>MAN’S</td>\n","      <td>mans</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(ARRIVAL, NNP)</td>\n","      <td>NNP</td>\n","      <td>ARRIVAL</td>\n","      <td>arrival</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <th>0</th>\n","      <th>0</th>\n","      <td>(The, DT)</td>\n","      <td>DT</td>\n","      <td>The</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">47</th>\n","      <th rowspan=\"5\" valign=\"top\">11</th>\n","      <th rowspan=\"5\" valign=\"top\">2</th>\n","      <th>5</th>\n","      <td>(of, IN)</td>\n","      <td>IN</td>\n","      <td>of</td>\n","      <td>of</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>(them, PRP)</td>\n","      <td>PRP</td>\n","      <td>them</td>\n","      <td>them</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>(until, IN)</td>\n","      <td>IN</td>\n","      <td>until</td>\n","      <td>until</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>(he, PRP)</td>\n","      <td>PRP</td>\n","      <td>he</td>\n","      <td>he</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>(dies., VB)</td>\n","      <td>VB</td>\n","      <td>dies.</td>\n","      <td>dies</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>48500 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51486ea8-6801-46fa-a296-9c982abd22bc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-51486ea8-6801-46fa-a296-9c982abd22bc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-51486ea8-6801-46fa-a296-9c982abd22bc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["british_corpus = CORPUS\n","british_corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"4ikrdcF_bmC2","executionInfo":{"status":"ok","timestamp":1683217834350,"user_tz":240,"elapsed":166,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"f6520fd1-164d-4307-a580-51a66021f1fb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                     pos_tuple  pos  \\\n","book_id chap_id para_num sent_num token_num                           \n","84      29      0        0        0                 (_To, NNP)  NNP   \n","                                  1                (Mrs., NNP)  NNP   \n","                                  2            (Saville,, NNP)  NNP   \n","                                  3           (England._, NNP)  NNP   \n","                1        0        0                 (St., NNP)  NNP   \n","...                                                        ...  ...   \n","18247   1       1241     8        39         (Verney—the, NNP)  NNP   \n","                                  40               (LAST, NNP)  NNP   \n","                                  41               (MAN., NNP)  NNP   \n","                1242     0        0                  (THE, DT)   DT   \n","                                  1                (END., NNP)  NNP   \n","\n","                                              token_str   term_str  \n","book_id chap_id para_num sent_num token_num                         \n","84      29      0        0        0                 _To         to  \n","                                  1                Mrs.        mrs  \n","                                  2            Saville,    saville  \n","                                  3           England._    england  \n","                1        0        0                 St.         st  \n","...                                                 ...        ...  \n","18247   1       1241     8        39         Verney—the  verneythe  \n","                                  40               LAST       last  \n","                                  41               MAN.        man  \n","                1242     0        0                 THE        the  \n","                                  1                END.        end  \n","\n","[330840 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-dbe90aab-5bce-4ea4-8ffa-6c6abf5aecc9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>pos_tuple</th>\n","      <th>pos</th>\n","      <th>token_str</th>\n","      <th>term_str</th>\n","    </tr>\n","    <tr>\n","      <th>book_id</th>\n","      <th>chap_id</th>\n","      <th>para_num</th>\n","      <th>sent_num</th>\n","      <th>token_num</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">84</th>\n","      <th rowspan=\"5\" valign=\"top\">29</th>\n","      <th rowspan=\"4\" valign=\"top\">0</th>\n","      <th rowspan=\"4\" valign=\"top\">0</th>\n","      <th>0</th>\n","      <td>(_To, NNP)</td>\n","      <td>NNP</td>\n","      <td>_To</td>\n","      <td>to</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(Mrs., NNP)</td>\n","      <td>NNP</td>\n","      <td>Mrs.</td>\n","      <td>mrs</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(Saville,, NNP)</td>\n","      <td>NNP</td>\n","      <td>Saville,</td>\n","      <td>saville</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(England._, NNP)</td>\n","      <td>NNP</td>\n","      <td>England._</td>\n","      <td>england</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <th>0</th>\n","      <th>0</th>\n","      <td>(St., NNP)</td>\n","      <td>NNP</td>\n","      <td>St.</td>\n","      <td>st</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">18247</th>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th rowspan=\"3\" valign=\"top\">1241</th>\n","      <th rowspan=\"3\" valign=\"top\">8</th>\n","      <th>39</th>\n","      <td>(Verney—the, NNP)</td>\n","      <td>NNP</td>\n","      <td>Verney—the</td>\n","      <td>verneythe</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>(LAST, NNP)</td>\n","      <td>NNP</td>\n","      <td>LAST</td>\n","      <td>last</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>(MAN., NNP)</td>\n","      <td>NNP</td>\n","      <td>MAN.</td>\n","      <td>man</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">1242</th>\n","      <th rowspan=\"2\" valign=\"top\">0</th>\n","      <th>0</th>\n","      <td>(THE, DT)</td>\n","      <td>DT</td>\n","      <td>THE</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(END., NNP)</td>\n","      <td>NNP</td>\n","      <td>END.</td>\n","      <td>end</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>330840 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbe90aab-5bce-4ea4-8ffa-6c6abf5aecc9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dbe90aab-5bce-4ea4-8ffa-6c6abf5aecc9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dbe90aab-5bce-4ea4-8ffa-6c6abf5aecc9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from google.colab import files\n","\n","british_corpus.to_csv('nikita_corpus.csv', encoding = 'utf-8-sig')\n","\n","files.download('nikita_corpus.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"dx4AmPWKerxH","executionInfo":{"status":"ok","timestamp":1682226073367,"user_tz":240,"elapsed":1696,"user":{"displayName":"Nikita A.","userId":"15034559426083067463"}},"outputId":"0a67ae54-f2ce-4e60-da8a-e9e50e486c0c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b5e0f2ea-71b8-4f57-877a-6618408b0f2d\", \"nikita_corpus.csv\", 15931004)"]},"metadata":{}}]}]}
